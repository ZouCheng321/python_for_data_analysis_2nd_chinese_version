{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import interpolate\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Factored Sampling\n",
    "This part is mostly illustrative, so you just have a few TODO's.\n",
    "Condensation is actually Factored Sampling, but applied iteratively to\n",
    "a sequence of observations, and incorporating a motion model. So here,\n",
    "time will essentially stand still: given a SINGLE set of \"observations\",\n",
    "can you estimate the posterior probabilities? \n",
    "Observations will be simulated: The real 2D distribution we would like \n",
    "to estimate is the red channel of an abstract image (provided). But\n",
    "pretend you can't look at the whole image, and can only take measurements\n",
    "here and there. \n",
    "Below, you have the code for factored sampling, but note the comment\n",
    "\"Loop from here\". Looping will only be needed in Part b because we will have a\n",
    "changing state, so each loop will advance from t to t+1. Here, you can\n",
    "abuse factored sampling a little, and loop \"in place\". If time were\n",
    "advancing and you had a real motion model, that would be Condensation.\n",
    "Observe: when you do this factored Resampling, more of the particles \n",
    "should be landing near the peaks in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('data/abstract.png')\n",
    "measurementsComprehensive = img[:,:,2]\n",
    "plt.imshow(measurementsComprehensive)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgHeight, imgWidth, colors = img.shape\n",
    "numParticles = 150;\n",
    "# Initialize which samples from \"last time\" we want to propagate: all of\n",
    "# them!:\n",
    "samples_to_propagate = range(0, numParticles)\n",
    "numDims_w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we randomly initialize some particles throughout the space of w.\n",
    "# The positions of such particles are quite close to the known initial\n",
    "# position:\n",
    "particles_old = np.random.rand(numParticles, numDims_w)\n",
    "#note: clipping here is just done to simplify plotting - we want everything to 'fit'\n",
    "particles_old[:,0] = particles_old[:,0] * imgHeight\n",
    "particles_old[:,1] = particles_old[:,1] * imgWidth\n",
    "\n",
    "weight_of_samples = np.ones((numParticles,1))\n",
    "# TO DO: normalize the weights (may be trivial this time)\n",
    "weight_of_samples = weight_of_samples #replace this \n",
    "\n",
    "# We loop here to see what factored resampling would look like.\n",
    "for iTime in range(10):\n",
    "    print('Iteration ', iTime, ':')\n",
    "    # TO DO: compute the cumulative sum of the weights\n",
    "    cum_hist_of_weights = np.linspace(0, 1, numParticles) # replace this \n",
    "\n",
    "    # Resample the old distribution (at time t-1), favoring samples that had\n",
    "    # a higher posterior probability\n",
    "    samples_to_propagate = np.zeros((numParticles,1), dtype=np.int32)\n",
    "    \n",
    "    # Pick random thresholds in the cumulative probability's range [0,1]: \n",
    "    some_threshes = np.random.rand(numParticles,1)\n",
    "\n",
    "    # For each random threshold, find which sample in the ordered set is the\n",
    "    # first one to push the cumulative probability above that threshold. \n",
    "    # E.g. if the cumulative histogram goes from 0.23 to 0.26 between the 17th\n",
    "    # and 18th samples in the old distribution, and the threshold is 0.234,\n",
    "    # then we'll want to propagate the 18th sample's w (i.e. particle #18).\n",
    "    for sampNum in range(numParticles): \n",
    "        thresh = some_threshes[sampNum]\n",
    "        for index in range(numParticles):\n",
    "            if cum_hist_of_weights[index] > thresh:\n",
    "                break\n",
    "        samples_to_propagate[sampNum] = index\n",
    "    \n",
    "    # Note: it's ok if some of the old particles get picked repeatedly, while\n",
    "    # others don't get picked at all.\n",
    "\n",
    "    # =================================================\n",
    "    # Visualize\n",
    "    # =================================================\n",
    "    plt.title('Cumulative histogram of probabilities for sorted list of particles')\n",
    "    plt.plot(np.zeros(numParticles), some_threshes,'b.')\n",
    "    plt.plot(range(0, numParticles), cum_hist_of_weights, 'rx-')\n",
    "    which_sample_ids = np.unique(samples_to_propagate)\n",
    "    how_many_of_each = np.bincount(np.ravel(samples_to_propagate))\n",
    "    for k in range(len(which_sample_ids)):\n",
    "        plt.plot(which_sample_ids[k], 0, 'bo-', markersize = 3 * how_many_of_each[k], markerfacecolor='white')\n",
    "    plt.xlabel('Indeces of all available samples, with larger blue circles for frequently re-sampled particles\\n(Iteration %01d)' % iTime)\n",
    "    plt.ylabel('Cumulative probability');\n",
    "    plt.show()\n",
    "    # =================================================\n",
    "    # =================================================\n",
    "\n",
    "    # Predict where the particles we sampled from the old distribution of \n",
    "    # state-space will go in the next time-step. This means we have to apply \n",
    "    # the motion model to each old sample.\n",
    "    particles_new = np.zeros_like(particles_old)\n",
    "    for particleNum in range(numParticles):\n",
    "        # TO DO: Incorporate some noise, e.g. Gaussian noise with std 10,\n",
    "        # into the current location (particles_old), to give a Brownian\n",
    "        # motion model.\n",
    "        particles_new[particleNum, :] =  particles_old[particleNum, :] # replace this \n",
    "        \n",
    "    measurementsComprehensive = img[:,:,2]\n",
    "    plt.imshow(measurementsComprehensive)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0, imgWidth])\n",
    "    axes.set_ylim([0, imgHeight])\n",
    "    # now draw the particles onto the image\n",
    "    plt.plot(particles_new[:,1], particles_new[:,0], 'rx')\n",
    "    plt.show()\n",
    "\n",
    "    # From here we incorporate the sensor measurement for the new state (time t):\n",
    "    # The new particles, accompanied with predicted locations in world state-space\n",
    "    # for time t, are missing their weights: how well does each particle\n",
    "    # explain the observation x_t?\n",
    "    for particleNum in range(numParticles):\n",
    "        # Convert the particle from state-space w to measurement-space x:\n",
    "        # Note: It is trivial in this case because both are in 2D space of image\n",
    "        # coordinates\n",
    "\n",
    "        # Within the loop, we evaluate the likelihood of each particle:\n",
    "        particle = particles_new[particleNum, :]\n",
    "        # Check that the predicted location is a place we can really evaluate\n",
    "        # the likelihood.\n",
    "        inFrame = particle[0] >= 1.0 and  particle[0] <= imgHeight and particle[1] >= 1.0 and particle[1] <= imgWidth\n",
    "        if inFrame:\n",
    "            interpolation_func = sp.interpolate.interp2d(np.arange(0, imgWidth),\n",
    "                                                         np.arange(0, imgHeight),\n",
    "                                                         measurementsComprehensive)\n",
    "            weight_of_samples[particleNum] = interpolation_func(particles_new[particleNum, 1], particles_new[particleNum, 0])\n",
    "        else:\n",
    "            weight_of_samples[particleNum] = 0.0\n",
    "\n",
    "    # TO DO: normalize the weights \n",
    "    weight_of_samples = weight_of_samples # replace this\n",
    "\n",
    "\n",
    "    # Now we're done updating the state for time t. \n",
    "    # For Condensation, just clean up and prepare for the next round of \n",
    "    # predictions and measurements:\n",
    "    particles_old = particles_new;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
