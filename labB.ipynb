{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (12.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Condensation\n",
    "\n",
    "Now, we will track a given shape (template) as it moves in a sequence of\n",
    "frames. Our shape/appearance model is trivial: just a template, given.\n",
    "We will only explore simple motion models. Such models may\n",
    "contain a variable that switches between cars and pedestrians, or traffic\n",
    "in different directions, or accelerating etc.\n",
    " \n",
    "Note: depending on the motion model, the state w will have different\n",
    "numbers of dimensions (not just 2, though that's the default). \n",
    "\n",
    "Most of this algorithm should be copied from Part a. An important\n",
    "difference is that now we'll actually compute the likelihood using the\n",
    "(provided) computeLikelihood() function.\n",
    "\n",
    "\n",
    "Complete reamining TO DO parts you didn't answer in Part a, run the code \n",
    "over the sequence, and observe the results. \n",
    "\n",
    "Then TO DO:\n",
    "- Try varying the number of particles: 2000, 500, 100,...\n",
    "- Change the state to have 2 more degrees of freedom: velX and velY.\n",
    "  (This will require you to change how state-predictions are made, and \n",
    "  how they are converted to measurement-space)\n",
    "- Visualize the top-scoring particles (more than just 1!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likelihood function is simple patch similarity\n",
    "#Note that below, we add a few 'corrections', such as\n",
    "#manually masking out the sky for better results\n",
    "def computeLikelihood(image, template):\n",
    "    #opencv's available methods - experiment with these\n",
    "    #careful what range the output is!\n",
    "    methods = [cv.TM_CCOEFF, cv.TM_CCOEFF_NORMED, cv.TM_CCORR,\n",
    "            cv.TM_CCORR_NORMED, cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]\n",
    "    \n",
    "    likelihood = cv.matchTemplate(image, template, methods[0])\n",
    "    #we manually eliminate incorrect matches in the sky\n",
    "    #since we know that there cannot be any cars in those areas\n",
    "    likelihood[:int(likelihood.shape[0] / 2), :] = 0.0\n",
    "    \n",
    "    #we can then pad to make this the size of the input image (for easier indexing)\n",
    "    pad_first = int(template.shape[0])\n",
    "    pad_second = int(template.shape[1])\n",
    "    pad_amounts = ((pad_first, pad_first), (pad_second, pad_second))\n",
    "    likelihood = np.pad(likelihood, pad_amounts, 'constant')\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load template and starting position ('pos'), which come from frame 871.\n",
    "template = sp.io.loadmat('data/Template.mat')['pixelsTemplate']\n",
    "\n",
    "#let's show the template\n",
    "print('We are matching this template with shape: ', template.shape)\n",
    "plt.imshow(template)\n",
    "plt.show()\n",
    "\n",
    "# Load images (to be comparable to matlab implementation)\n",
    "images = []\n",
    "iFrame = 0\n",
    "for frameNum in range(872, 894 + 1):\n",
    "    imageName = 'data/HillsRdSkipFrames_%07d.png' % frameNum\n",
    "    images.append(cv.imread(imageName))\n",
    "    plt.show()\n",
    "    iFrame += 1\n",
    "\n",
    "imgHeight, imgWidth, colors = images[0].shape\n",
    "numParticles = 2000;\n",
    "weight_of_samples = np.ones((numParticles,1))\n",
    "\n",
    "# TO DO: normalize the weights (may be trivial this time) [done]\n",
    "weight_of_samples = weight_of_samples #replace this \n",
    "\n",
    "# Initialize which samples from \"last time\" we want to propagate: all of\n",
    "# them!:\n",
    "samples_to_propagate = range(0, numParticles)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# NOT A TO DO: You don't need to change the code below, but eventually you may\n",
    "# want to vary the number of Dims. \n",
    "numDims_w = 4;\n",
    "# Here we randomly initialize some particles throughout the space of w:\n",
    "particles_old = np.random.rand(numParticles, numDims_w)\n",
    "particles_old[:,0] = particles_old[:,0] * imgHeight\n",
    "particles_old[:,1] = particles_old[:,1] * imgWidth\n",
    "particles_old[:,2] = 10.0 * np.random.randn(numParticles)\n",
    "particles_old[:,3] = 10.0 * np.random.randn(numParticles)\n",
    "# ============================\n",
    "\n",
    "for iTime in range(22):\n",
    "    print('Processing Frame', iTime)\n",
    "    # TO DO: compute the cumulative sume of the weights. [done]\n",
    "    cum_hist_of_weights = np.linspace(0, 1, numParticles) # replace this \n",
    "\n",
    "\n",
    "    # ==============================================================\n",
    "    # Resample the old distribution at time t-1, and select samples, favoring\n",
    "    # those that had a higher posterior probability.\n",
    "    # ==============================================================\n",
    "    samples_to_propagate = np.zeros(numParticles, dtype=np.int32)\n",
    "    \n",
    "    # Pick random thresholds in the cumulative probability's range [0,1]:\n",
    "    some_threshes = np.random.rand(numParticles)\n",
    "\n",
    "\n",
    "    # For each random threshold, find which sample in the ordered set is\n",
    "    # the first one to push the cumulative probability above that\n",
    "    # threshold, e.g. if the cumulative histogram goes from 0.23 to 0.26\n",
    "    # between the 17th and 18th samples in the old distribution, and the\n",
    "    # threshold is 0.234, then we'll want to propagate the 18th sample's w\n",
    "    # (i.e. particle #18).\n",
    "    \n",
    "    for sampNum in range(numParticles): \n",
    "        thresh = some_threshes[sampNum]\n",
    "        for index in range (numParticles):\n",
    "            if cum_hist_of_weights[index] > thresh:\n",
    "                break\n",
    "        samples_to_propagate[sampNum] = index\n",
    "    \n",
    "    # Note: it's ok if some of the old particles get picked repeatedly, while\n",
    "    # others don't get picked at all.\n",
    "\n",
    "    # Predict where the particles from the old distribution of \n",
    "    # state-space will go in the next time-step. This means we have to apply \n",
    "    # the motion model to each old sample.\n",
    "\n",
    "    particles_new = np.zeros(particles_old.shape)\n",
    "    for particleNum in range(numParticles):      \n",
    "        # TO DO: Incorporate some noise, e.g. Gaussian noise with std 10,\n",
    "        # into the current location (particles_old), to give a Brownian\n",
    "        # motion model with changing velocity. \n",
    "\n",
    "        particles_new[particleNum, 2:] = particles_old[particleNum, 2:] #replace this\n",
    "        \n",
    "        particles_new[particleNum, :2] = particles_old[particleNum, :2] #replace this\n",
    "        particles_new[particleNum, :2] = np.round(particles_new[particleNum, :2]) # Round the particles_new to simplify Likelihood evaluation.\n",
    "    \n",
    "    # TO DO: Not initially, but change the motion model above to have\n",
    "    # different degrees of freedom, and optionally completely different\n",
    "    # motion models.\n",
    "    \n",
    "    #calculate likelihood function\n",
    "    likelihood = computeLikelihood(images[iTime], template)\n",
    "    \n",
    "    #plot results\n",
    "    f, axarr = plt.subplots(1, 2)\n",
    "    axarr[0].imshow(images[iTime])\n",
    "    axarr[0].set_title('Particles')\n",
    "    # now draw the particles onto the image\n",
    "    axarr[0].plot(particles_new[:,1], particles_new[:,0], 'rx')\n",
    "    \n",
    "    #Plot the best scoring particles\n",
    "    indices = np.argsort(weight_of_samples)\n",
    "    bestScoringParticles = particles_new[np.squeeze(indices[-15:]), :]\n",
    "    plt.plot(bestScoringParticles[:,1], bestScoringParticles[:,0], 'gx')\n",
    "    \n",
    "    #plot the likelihood\n",
    "    axarr[1].imshow(likelihood)\n",
    "    axarr[1].set_title('Likelihood')\n",
    "    plt.show()\n",
    "    \n",
    "    # From here we incorporate the data for the new state (time t):\n",
    "    # The new particles accompanying predicted locations in state-space\n",
    "    # for time t, are missing their weights: how well does each particle\n",
    "    # explain the observations x_t?\n",
    "    for particleNum in range(numParticles):\n",
    "\n",
    "        # Convert the particle from state-space w to measurement-space x:\n",
    "        # Note: that step is trivial here since both are in 2D space of image\n",
    "        # coordinates\n",
    "\n",
    "        # Within the loop, we evaluate the likelihood of each particle:\n",
    "        particle = particles_new[particleNum, :]\n",
    "        # Check that the predicted location is a place we can really evaluate\n",
    "        # the likelihood.\n",
    "        inFrame = particle[0] >= 0.0 and  particle[0] <= imgHeight and particle[1] >= 0.0 and particle[1] <= imgWidth\n",
    "        if inFrame:\n",
    "            minX = particle[1]\n",
    "            minY = particle[0]\n",
    "\n",
    "            weight_of_samples[particleNum] = likelihood[int(minY), int(minX)]\n",
    "        else:\n",
    "            weight_of_samples[particleNum] = 0.0\n",
    "\n",
    "    # TO DO: normalize the weights [done]\n",
    "    weight_of_samples = weight_of_samples #replace this\n",
    "\n",
    "\n",
    "    # Now we're done updating the state for time t. \n",
    "    # For Condensation, just clean up and prepare for the next round of \n",
    "    # predictions and measurements:\n",
    "    particles_old = particles_new\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
